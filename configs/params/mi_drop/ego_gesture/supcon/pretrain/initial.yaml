n_views: &n_views 2
in_channels: &in_channels 3
seq_len: &seq_len 8
rm_global_scale: &rm_global_scale On
batch_size: 32
workers: 4
total_epochs: 200
step_per_epoch: &step_per_epoch On
step_per_batch: &step_per_batch Off
log_freq:
    train: 20
    val: null

model:
    name: 'dg_sta_contrastive'
    in_channels: *in_channels
    n_heads: 8 # number of attention heads
    d_head: 32 # 32, dimension of attention heads
    d_feat: 128 # 128, feature dimension
    seq_len: *seq_len # sequence length
    n_joints: 42 # number of joints 21 x 2 hands
    dropout: 0.2
    d_head_contrastive: 32    


transforms:
    train:
        # random_time_interpolation:
        #     prob: 0.5

        stratified_sample:
            n_samples: *seq_len

        # random_scale:
        #     lim: [0.8, 1.2] #[0.1, 5.0] #[0.8, 1.2]

        # # random_noise:
        # #     lim: 0.01
        # #     rm_global_scale: *rm_global_scale

        # random_translation:
        #     x: 0.1
        #     y: 0.1
        #     z: 0.1

        # random_rotation: # degrees
        #     x: 5.0
        #     y: 5.0
        #     z: 5.0

        # center_by_index:
        #     ind: 0 # wrist
        #     n_hands: 2
            

    val: &transforms_inference
        stratified_sample:
            n_samples: *seq_len

        # center_by_index:
        #     ind: 0 # wrist
        #     n_hands: 2         
    testval: *transforms_inference  
    testtrain: *transforms_inference 
    test: *transforms_inference


loss:
    contrastive: 
        weight: 1.0
        n_views: *n_views
        is_supervised: On
        temperature: 0.07


optimizer:
    name: 'adam'
    lr: &lr 0.001
    betas: [0.9, 0.999]
    scheduler:
        name: 'linear'
        lr: *lr
        start_factor: 1
        end_factor: 0.001 # 0.001 - 76.4
        step_per_batch: *step_per_batch
        step_per_epoch: *step_per_epoch        
